# 劳动力市场中技能溢价与薪酬决定机制研究——基于多源招聘平台数据的实证分析

## 1. 研究思路与策略

本项目采用“双平台对比 + 多维分析”的实证策略，旨在从宏观分布与微观机制两个层面全面解析劳动力市场的薪酬决定因素：

*   **数据源选取**：
    *   **智联招聘（广度优先）**：利用其列表页信息密度高的特点，侧重于按城市进行大规模数据采集，用于构建宏观薪资分布图谱和地理特征分析。
    *   **BOSS直聘（深度优先）**：克服其严格的反爬机制，深入采集包含详细职位描述（JD）和技能标签的微观数据，用于测度具体的“技能溢价”。
*   **时间窗口**：2025年12月01日 - 12月07日。
*   **目标领域**：聚焦于商科相关的金融、数据分析等职业。

---

## 2. 爬虫模块：数据采集与工程实现

### 2.1 智联招聘 (`zlzp/`)

该模块包含两个针对性优化的脚本，分别处理通用爬取和特定算法岗位的深度爬取。

*   **核心逻辑 (`zhilianzhaopin_spider.py`)**：
    *   **技术栈**：基于 `Selenium` 模拟浏览器行为，配合 `Pandas` 进行实时数据处理。
    *   **页面交互**：使用 `WebDriverWait` 配合 CSS Selector (`.joblist-box__item`) 精准定位动态加载的职位卡片。
    *   **字段提取**：解析职位名称、薪资、公司标签等基础信息，并从混合文本中提取学历与经验要求。
    *   **容错与持久化**：采用**增量保存**策略，每爬取一页即追加写入 Excel/CSV，支持断点续传，最大程度防止数据丢失。

*   **进阶策略 (`算法爬取.py`)**：
    *   **反爬增强**：引入 `selenium-stealth` 库抹除 WebDriver 特征，并注入自定义 JavaScript 脚本以规避高阶反爬检测。
    *   **数据源优化**：优先尝试读取 DOM 节点的 `sensorsdata` 属性（后端预格式化的 JSON 数据），解析更准确；若失败则自动降级为传统的 XPath/CSS 文本提取。
    *   **拟人化操作**：实现了符合人类行为特征的随机滚动 (`window.scrollBy`) 和非固定延时。
    *   **数据存储**：采用**批量保存**策略，任务结束后统一写入 CSV（代码中预留了 MongoDB 入库接口，默认关闭）。

通过对网页网络请求的分析（如图1所示），BOSS 直聘采用了前后端分离架构，列表数据通过加密 API 传输，且具备极严的风控机制（`zp_token` 校验、IP 封禁）。为此，本项目设计了**“API 逆向 + 详情页拼接”的交互式爬虫方案** (`final_boss_crawl.ipynb`)。

*   **策略一：列表页 API 逆向（效率优先）**
    *   **发现**：观察网络流量发现，职位列表数据由后端接口 `wapi/zpgeek/search/joblist.json` 动态下发。
    *   **实现**：脚本直接构造 HTTP 请求调用该接口。相比传统的 Selenium 渲染，这种方式能直接获取结构化的 JSON 数据（含加密的 `encryptJobId`、精确的薪资结构如“15-30K·16薪”、官方技能标签），避免了 HTML 解析的误差，且速度提升显著。

*   **策略二：详情页 HTML 解析（深度优先）**
    *   **必要性**：API 返回的数据仅包含摘要，而核心的**职位描述 (JD)**（如截图左侧的“岗位职责”、“任职要求”）仅在详情页 HTML 中完整渲染。这是后续 NLP 分析（提取硬技能、计算相似度）的必要语料。
    *   **实现**：利用列表页获取的 `job_id` 拼接详情页 URL，使用 `BeautifulSoup` 定向提取文本内容，与列表数据进行合并。

*   **策略三：交互式风控规避（稳定性优先）**
    *   **Cookie 注入**：鉴于 `zp_token` 的生成算法极其复杂，脚本通过加载 `cookie.txt` 复用浏览器端的有效会话，绕过复杂的登录验证码。
    *   **人机协同**：采用 Jupyter Notebook 交互式运行。一旦触发 403 封禁或验证码拦截，程序会自动保存当前进度（`crawl_state.txt`）并暂停。用户只需在浏览器手动通过验证并更新 Cookie，即可实现**断点续传**

---

## 3. 数据分析模块：从宏观统计到微观计量

### 3.1 宏观分布分析 (`智联招聘统计分析.ipynb`)

该模块基于智联招聘的大规模数据，侧重于描述性统计和行业不平等度量。

1.  **数据清洗与标准化**：
    *   将“1.5-2.5万/月”等区间字符串清洗为数值型均值。
    *   基于关键词将职位划分为“量化”、“投行”、“数据分析”、“财务”等核心类别。
2.  **多维可视化**：
    *   **城市薪资对比**：绘制柱状图展示北上广深及新一线城市的薪资梯队。
    *   **岗位竞争力**：对比不同细分领域的平均薪资，识别高薪赛道。
    *   **热力图分析**：构建“岗位类型-城市”的薪资热力图，定位特定城市的优势产业。
    *   **学历构成**：使用堆积柱状图展示各城市招聘岗位的学历门槛差异。
3.  **不平等度量**：
    *   计算各岗位类型的**基尼系数 (Gini Coefficient)**，并绘制**洛伦兹曲线 (Lorenz Curve)**，直观展示“量化”、“财务”等不同职业内部的收入分配差距（如是否存在严重的“赢家通吃”现象）。

### 3.2 微观技能溢价分析 (`boss直聘分析.ipynb`)

该模块结合自然语言处理（NLP）与计量经济学方法，深入探究“技能”对薪资的边际贡献。

1.  **NLP 与特征工程**：
    *   **文本向量化**：使用 `jieba` 分词生成词云，并利用 `TfidfVectorizer` 将非结构化 JD 转化为 TF-IDF 矩阵。
    *   **LLM 智能清洗**：创新性地接入 **阿里通义千问 API (Qwen-Plus)**，自动识别并剔除“五险一金”、“团队氛围”等无业务价值的行政废话，保留高价值的硬技能关键词。
    *   **技能相似度测度**：构建“高薪职位原型”（Top 10% 高薪岗位的 JD 聚合），计算每个职位与该原型的**余弦相似度**，作为衡量技能含金量的核心指标。

2.  **计量建模与因果推断**：
    *   **多元线性回归 (OLS)**：建立模型 $ Salary = \alpha + \beta_1 Education + \beta_2 Experience + \beta_3 Similarity + \epsilon $，在控制学历和经验后，验证技能相似度对薪资的显著正向影响。
    *   **边际贡献可视化**：绘制堆积条形图，量化分解学历、经验和技能对薪资增幅的具体贡献值。
    *   **倾向得分匹配 (PSM)**：使用 Logistic 回归估算倾向得分，进行 1:1 近邻匹配，消除样本选择偏差，计算处理组（高技能相似度）与对照组的平均薪资差异 (ATT)。

3.  **异质性与稳健性**：
    *   **异质性分析**：探讨技能溢价在**学历**（交互项）、**公司规模**、**职业阶段**以及**城市等级**（一线/新一线）等不同群体中的差异。
    *   **安慰剂检验**：随机打乱技能相似度得分进行 500 次回归模拟，验证真实回归系数的显著性。

4.  **地理空间分析**：
    * 利用 folium 库绘制交互式地图，将薪资数据映射到地理坐标，直观展示高薪职位的空间集聚效应。

## 项目文件结构

### 核心分析脚本
*   `智联招聘统计分析.ipynb` - 宏观分布与行业不平等分析
*   `boss直聘分析.ipynb` - 技能溢价的微观计量研究

### 数据采集模块

**BOSS直聘爬虫** (`boss/`)
*   `final_boss_crawl.ipynb` - 交互式爬虫（API逆向 + 详情页拼接）
*   `joblist_1.xlsx`, `joblist_2.xlsx` - 爬取的职位原始数据
*   `cookie.txt` - 会话管理文件
*   `图1.png` - 网页解析示意图

**智联招聘爬虫** (`zlzp/`)
*   `zhilianzhaopin_spider.py` - 核心爬虫（Selenium+WebDriverWait）
*   `算法爬取.py` - 反爬增强脚本（selenium-stealth + sensorsdata）
*   **城市数据目录** - 按城市聚合的职位数据
    *   `上海/`，`北京/`，`广州/`, `深圳/`, `杭州/`, `南京/` - 12 个岗位类别 × Excel 文件

### 辅助文件
*   `README.md` - 项目文档
*   `temp.py` - 临时脚本
*   `.gitignore` - Git 配置