{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ab1a2d0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import time\n",
    "import random\n",
    "from bs4 import BeautifulSoup\n",
    "import datetime\n",
    "import re\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "80401e07",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BossSpiderAPI:\n",
    "    \"\"\"BOSSç›´è˜çˆ¬è™«ï¼šçˆ¬å–é‡‘èç›¸å…³èŒä½åˆ—è¡¨å’Œè¯¦æƒ…\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.base_url = \"https://www.zhipin.com/wapi/zpgeek/search/joblist.json\"\n",
    "        \n",
    "        # ä»æ–‡ä»¶è¯»å–Cookie\n",
    "        cookie = self._load_cookie_from_file()\n",
    "        \n",
    "        self.headers = {\n",
    "            \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36\",\n",
    "            \"Referer\": \"https://www.zhipin.com/web/geek/job\",\n",
    "            \"Accept\": \"application/json, text/plain, */*\",\n",
    "            \"Cookie\": cookie\n",
    "        }\n",
    "        \n",
    "        # é‡‘èå…³é”®è¯æ­£åˆ™è¡¨è¾¾å¼\n",
    "        self.finance_pattern = re.compile(\n",
    "            r\"(é‡‘è|é“¶è¡Œ|åˆ¸å•†|è¯åˆ¸|åŸºé‡‘|ä¿é™©|æœŸè´§|ä¿¡æ‰˜|æŠ•è¡Œ|èµ„ç®¡|ç†è´¢|è´¢å¯Œ|å°è´·|æ¶ˆé‡‘|ä¿ç†|ç§Ÿèµ|å¾ä¿¡|æ¸…ç®—|æ”¯ä»˜|äº’é‡‘|é‡‘èç§‘æŠ€|FinTech)\",\n",
    "            re.IGNORECASE\n",
    "        )\n",
    "        \n",
    "        self.progress_file = \"job_data_IN_PROGRESS.xlsx\"\n",
    "        self.state_file = \"crawl_state.txt\"\n",
    "        self.detail_state_file = \"detail_state.txt\"\n",
    "        self.data_list = []\n",
    "        self.seen_ids = set()\n",
    "\n",
    "    def _load_cookie_from_file(self, filepath=\"cookie.txt\"):\n",
    "        \"\"\"ä»cookie.txtè¯»å–Cookie\"\"\"\n",
    "        try:\n",
    "            with open(filepath, 'r', encoding='utf-8') as f:\n",
    "                cookie = f.read().strip()\n",
    "                lines = [line.strip() for line in cookie.split('\\n') \n",
    "                        if line.strip() and not line.strip().startswith('//')]\n",
    "                cookie = ''.join(lines)\n",
    "                cookie = cookie.encode('ascii', 'ignore').decode('ascii')\n",
    "                \n",
    "                if cookie:\n",
    "                    print(f\"âœ… æˆåŠŸåŠ è½½ Cookie\")\n",
    "                    return cookie\n",
    "                else:\n",
    "                    print(f\"âš ï¸ Cookieæ–‡ä»¶ä¸ºç©º\")\n",
    "                    return \"\"\n",
    "        except FileNotFoundError:\n",
    "            print(f\"âš ï¸ æœªæ‰¾åˆ°cookie.txt\")\n",
    "            return \"\"\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ è¯»å–å¤±è´¥: {e}\")\n",
    "            return \"\"\n",
    "\n",
    "    def _simple_sleep(self, min_sec=2.0, max_sec=4.0):\n",
    "        \"\"\"éšæœºç­‰å¾…\"\"\"\n",
    "        time.sleep(random.uniform(min_sec, max_sec))\n",
    "\n",
    "    def is_finance_related(self, job):\n",
    "        \"\"\"åˆ¤æ–­èŒä½æ˜¯å¦ä¸é‡‘èç›¸å…³\"\"\"\n",
    "        check_fields = [\n",
    "            job.get(\"jobName\", \"\"),\n",
    "            job.get(\"brandName\", \"\"),\n",
    "            job.get(\"brandIndustry\", \"\"),\n",
    "        ]\n",
    "        combined_text = \" \".join(str(f) for f in check_fields if f)\n",
    "        return bool(self.finance_pattern.search(combined_text))\n",
    "\n",
    "    def load_crawl_state(self, keywords):\n",
    "        \"\"\"åŠ è½½åˆ—è¡¨çˆ¬å–çš„æ–­ç‚¹è¿›åº¦\"\"\"\n",
    "        if not os.path.exists(self.state_file):\n",
    "            return 0, 1\n",
    "        \n",
    "        try:\n",
    "            with open(self.state_file, 'r', encoding='utf-8') as f:\n",
    "                lines = f.readlines()\n",
    "                if len(lines) < 2: \n",
    "                    return 0, 1\n",
    "                keyword = lines[0].strip()\n",
    "                page = int(lines[1].strip())\n",
    "                \n",
    "                if keyword in keywords:\n",
    "                    idx = keywords.index(keyword)\n",
    "                    print(f\"ğŸ“ æ¢å¤è¿›åº¦ï¼š{keyword} ç¬¬ {page} é¡µ\")\n",
    "                    return idx, page\n",
    "                else:\n",
    "                    return 0, 1\n",
    "        except Exception as e:\n",
    "            print(f\"âš ï¸ è¯»å–çŠ¶æ€å¤±è´¥ï¼Œä»å¤´å¼€å§‹: {e}\")\n",
    "            return 0, 1\n",
    "\n",
    "    def save_crawl_state(self, keyword, page):\n",
    "        \"\"\"ä¿å­˜åˆ—è¡¨çˆ¬å–çš„æ–­ç‚¹è¿›åº¦\"\"\"\n",
    "        try:\n",
    "            with open(self.state_file, 'w', encoding='utf-8') as f:\n",
    "                f.write(f\"{keyword}\\n{page}\")\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    def load_detail_state(self):\n",
    "        \"\"\"åŠ è½½è¯¦æƒ…çˆ¬å–çš„æ–­ç‚¹è¿›åº¦\"\"\"\n",
    "        if not os.path.exists(self.detail_state_file):\n",
    "            return 0\n",
    "        try:\n",
    "            with open(self.detail_state_file, 'r', encoding='utf-8') as f:\n",
    "                index = int(f.read().strip())\n",
    "                print(f\"ğŸ“ æ¢å¤è¯¦æƒ…è¿›åº¦ï¼šä»ç¬¬ {index + 1} æ¡å¼€å§‹\")\n",
    "                return index\n",
    "        except Exception:\n",
    "            return 0\n",
    "\n",
    "    def save_detail_state(self, index):\n",
    "        \"\"\"ä¿å­˜è¯¦æƒ…çˆ¬å–çš„æ–­ç‚¹è¿›åº¦\"\"\"\n",
    "        try:\n",
    "            with open(self.detail_state_file, 'w', encoding='utf-8') as f:\n",
    "                f.write(str(index))\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    def fetch_data(self, query=\"é‡‘è\", start_page=1, max_pages=5):\n",
    "        \"\"\"çˆ¬å–èŒä½åˆ—è¡¨\"\"\"\n",
    "        consecutive_fail = 0\n",
    "        \n",
    "        for page in range(start_page, max_pages + 1):\n",
    "            print(f\"\\nğŸ” [{query}] ç¬¬ {page} é¡µ...\")\n",
    "            \n",
    "            params = {\n",
    "                \"scene\": \"1\", \"query\": query, \"city\": \"100010000\",\n",
    "                \"page\": page, \"pageSize\": 30\n",
    "            }\n",
    "            \n",
    "            try:\n",
    "                resp = requests.post(self.base_url, headers=self.headers, data=params, timeout=15)\n",
    "                result = resp.json()\n",
    "                code = result.get(\"code\")\n",
    "                \n",
    "                # 37æ˜¯é£æ§è§¦å‘\n",
    "                if code == 37:\n",
    "                    print(f\"âš ï¸ é£æ§è§¦å‘ï¼Œé€€å‡º\")\n",
    "                    self.save_crawl_state(query, page)\n",
    "                    self.save_progress()\n",
    "                    return False\n",
    "                    \n",
    "                if code not in (0, \"0\", None):\n",
    "                    print(f\"âš ï¸ å¼‚å¸¸å“åº” code={code}\")\n",
    "                    consecutive_fail += 1\n",
    "                    if consecutive_fail >= 5:\n",
    "                        break\n",
    "                    self._simple_sleep(5, 10)\n",
    "                    continue\n",
    "                    \n",
    "                consecutive_fail = 0\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"âŒ è¯·æ±‚å¼‚å¸¸: {e}\")\n",
    "                consecutive_fail += 1\n",
    "                if consecutive_fail >= 5:\n",
    "                    break\n",
    "                self._simple_sleep(3, 6)\n",
    "                continue\n",
    "\n",
    "            job_list = result.get(\"zpData\", {}).get(\"jobList\", [])\n",
    "            if not job_list:\n",
    "                print(\"âš ï¸ æ— æ›´å¤šæ•°æ®\")\n",
    "                break\n",
    "            \n",
    "            finance_count = 0\n",
    "            for job in job_list:\n",
    "                job_id = job.get(\"encryptJobId\")\n",
    "                if not job_id or job_id in self.seen_ids:\n",
    "                    continue\n",
    "                if not self.is_finance_related(job):\n",
    "                    continue\n",
    "                \n",
    "                finance_count += 1\n",
    "                self.data_list.append({\n",
    "                    \"èŒä½\": job.get(\"jobName\"),\n",
    "                    \"å…¬å¸\": job.get(\"brandName\"),\n",
    "                    \"è–ªèµ„\": job.get(\"salaryDesc\"),\n",
    "                    \"åœ°åŒº\": job.get(\"cityName\"),\n",
    "                    \"ç»éªŒ\": job.get(\"jobExperience\"),\n",
    "                    \"å­¦å†\": job.get(\"jobDegree\"),\n",
    "                    \"å…¬å¸è§„æ¨¡\": job.get(\"brandScaleName\"),\n",
    "                    \"è¡Œä¸š\": job.get(\"brandIndustry\"),\n",
    "                    \"ç¦åˆ©æ ‡ç­¾\": \",\".join(job.get(\"welfareList\", []) or []),\n",
    "                    \"æŠ€èƒ½æ ‡ç­¾\": \",\".join(job.get(\"skills\", []) or []),\n",
    "                    \"èŒä½æè¿°\": \"\",\n",
    "                    \"job_id\": job_id\n",
    "                })\n",
    "                self.seen_ids.add(job_id)\n",
    "            \n",
    "            print(f\"âœ… æœ¬é¡µ: {finance_count} | ç´¯è®¡: {len(self.data_list)}\")\n",
    "            \n",
    "            self.save_crawl_state(query, page + 1)\n",
    "            self.save_progress()\n",
    "            self._simple_sleep(2.0, 4.0)\n",
    "        \n",
    "        return True\n",
    "\n",
    "    def get_job_detail(self, job_id):\n",
    "        \"\"\"çˆ¬å–èŒä½è¯¦æƒ…é¡µ\"\"\"\n",
    "        url = f\"https://www.zhipin.com/job_detail/{job_id}.html\"\n",
    "        \n",
    "        try:\n",
    "            resp = requests.get(url, headers=self.headers, timeout=15)\n",
    "            \n",
    "            if resp.status_code == 404:\n",
    "                return \"èŒä½å·²å…³é—­\"\n",
    "            if resp.status_code == 403:\n",
    "                print(f\"  âš ï¸ 403ç¦æ­¢è®¿é—®\")\n",
    "                return None\n",
    "            if resp.status_code != 200:\n",
    "                print(f\"  âš ï¸ çŠ¶æ€ç : {resp.status_code}\")\n",
    "                return None\n",
    "            \n",
    "            # æ£€æŸ¥æ˜¯å¦è¢«é‡å®šå‘åˆ°éªŒè¯é¡µé¢\n",
    "            if 'security-check' in resp.url or 'verify' in resp.url:\n",
    "                print(f\"  âš ï¸ è§¦å‘å®‰å…¨éªŒè¯\")\n",
    "                return None\n",
    "            \n",
    "            soup = BeautifulSoup(resp.text, 'html.parser')\n",
    "            \n",
    "            # å¤šä¸ªé€‰æ‹©å™¨å°è¯•å®šä½èŒä½æè¿°\n",
    "            selectors = [\n",
    "                '.job-sec-text', '.job-detail-section .text', '.job-sec .text',\n",
    "                '.job-description', '.job-detail .text', '[class*=\"job-sec\"] .text'\n",
    "            ]\n",
    "            \n",
    "            for selector in selectors:\n",
    "                desc_tag = soup.select_one(selector)\n",
    "                if desc_tag and desc_tag.text.strip():\n",
    "                    return desc_tag.text.strip()\n",
    "            \n",
    "            return \"\"\n",
    "            \n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"  âš ï¸ è¯·æ±‚å¼‚å¸¸: {e}\")\n",
    "            return None\n",
    "\n",
    "    def load_progress(self):\n",
    "        \"\"\"ä»è¿›åº¦æ–‡ä»¶æ¢å¤å·²çˆ¬æ•°æ®\"\"\"\n",
    "        print(f\"\\nğŸ”„ æ­£åœ¨åŠ è½½ {self.progress_file}...\")\n",
    "        try:\n",
    "            df = pd.read_excel(self.progress_file)\n",
    "            df['èŒä½æè¿°'] = df['èŒä½æè¿°'].fillna('')\n",
    "            df['job_id'] = df['job_id'].fillna('')\n",
    "            \n",
    "            self.data_list = df.to_dict('records')\n",
    "            self.seen_ids = {item['job_id'] for item in self.data_list if item.get('job_id')}\n",
    "            \n",
    "            print(f\"âœ… å·²åŠ è½½ {len(self.data_list)} æ¡è®°å½•\")\n",
    "            \n",
    "        except FileNotFoundError:\n",
    "            print(\"â„¹ï¸ æœªæ‰¾åˆ°è¿›åº¦æ–‡ä»¶ï¼Œå°†åˆ›å»ºæ–°æ–‡ä»¶\")\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ åŠ è½½å¤±è´¥: {e}\")\n",
    "\n",
    "    def save_progress(self):\n",
    "        \"\"\"ä¿å­˜æ•°æ®åˆ°è¿›åº¦æ–‡ä»¶\"\"\"\n",
    "        if not self.data_list:\n",
    "            return\n",
    "        try:\n",
    "            df = pd.DataFrame(self.data_list)\n",
    "            df.to_excel(self.progress_file, index=False, na_rep='')\n",
    "        except Exception as e:\n",
    "            print(f\"  âŒ ä¿å­˜å¤±è´¥: {e}\")\n",
    "\n",
    "    def fetch_all_details_resumable(self, start_index=0):\n",
    "        \"\"\"çˆ¬å–æ‰€æœ‰èŒä½çš„è¯¦æƒ…æè¿°\"\"\"\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"ğŸš€ å¼€å§‹çˆ¬å–èŒä½è¯¦æƒ…...\")\n",
    "        if start_index > 0:\n",
    "            print(f\"ğŸ“ è·³è¿‡å‰ {start_index} æ¡ï¼Œä»ç¬¬ {start_index + 1} æ¡å¼€å§‹\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        fetched_count = 0\n",
    "        consecutive_fail = 0\n",
    "        total_items = len(self.data_list)\n",
    "        \n",
    "        for i in range(start_index, total_items):\n",
    "            item = self.data_list[i]\n",
    "            \n",
    "            # è·³è¿‡å·²æœ‰æè¿°çš„èŒä½\n",
    "            desc_str = str(item.get(\"èŒä½æè¿°\", \"\")).strip()\n",
    "            if desc_str and desc_str.lower() != 'nan' and len(desc_str) > 10:\n",
    "                continue\n",
    "            \n",
    "            print(f\"  ({i+1}/{total_items}) {item['èŒä½']} - {item['å…¬å¸']}\")\n",
    "            \n",
    "            job_id = item.get(\"job_id\")\n",
    "            if not job_id:\n",
    "                continue\n",
    "            \n",
    "            new_desc = self.get_job_detail(job_id)\n",
    "            \n",
    "            if new_desc is None:\n",
    "                consecutive_fail += 1\n",
    "                print(f\"  âŒ çˆ¬å–å¤±è´¥ (è¿ç»­: {consecutive_fail})\")\n",
    "                self.save_detail_state(i)\n",
    "                \n",
    "                # è¿ç»­å¤±è´¥3æ¬¡åˆ™è§¦å‘é£æ§ä¿æŠ¤\n",
    "                if consecutive_fail >= 3:\n",
    "                    print(f\"\\n  ğŸ›‘ è§¦å‘é£æ§ä¿æŠ¤ï¼Œåœæ­¢è¿è¡Œ\")\n",
    "                    self.save_progress()\n",
    "                    return i\n",
    "                \n",
    "                self._simple_sleep(5.0, 10.0)\n",
    "                \n",
    "            elif new_desc == \"\":\n",
    "                consecutive_fail = 0\n",
    "                item[\"èŒä½æè¿°\"] = \"[æœªè·å–åˆ°æè¿°]\"\n",
    "                print(f\"  âš ï¸ æœªæ‰¾åˆ°æè¿°\")\n",
    "                self._simple_sleep(3.0, 5.0)\n",
    "                \n",
    "            else:\n",
    "                consecutive_fail = 0\n",
    "                item[\"èŒä½æè¿°\"] = new_desc\n",
    "                fetched_count += 1\n",
    "                print(f\"  âœ… æˆåŠŸ\")\n",
    "                \n",
    "                # æ¯5ä¸ªä¿å­˜ä¸€æ¬¡è¿›åº¦\n",
    "                if fetched_count % 5 == 0:\n",
    "                    self.save_progress()\n",
    "                    self.save_detail_state(i + 1)\n",
    "                \n",
    "                self._simple_sleep(4.0, 8.0)\n",
    "        \n",
    "        self.save_progress()\n",
    "        self.save_detail_state(total_items)\n",
    "        \n",
    "        print(\"\\nâœ… è¯¦æƒ…çˆ¬å–å®Œæ¯•\")\n",
    "        if os.path.exists(self.detail_state_file):\n",
    "            os.remove(self.detail_state_file)\n",
    "        \n",
    "        return -1\n",
    "\n",
    "    def save_final_excel(self):\n",
    "        \"\"\"ä¿å­˜ä¸ºæœ€ç»ˆçš„Excelæ–‡ä»¶\"\"\"\n",
    "        if not self.data_list:\n",
    "            return\n",
    "        \n",
    "        final_data = [item.copy() for item in self.data_list]\n",
    "        for item in final_data:\n",
    "            item.pop('job_id', None)\n",
    "                \n",
    "        df = pd.DataFrame(final_data)\n",
    "        filename = f\"é‡‘èè¡Œä¸šå²—ä½_å®Œæ•´ç‰ˆ_{datetime.datetime.now().strftime('%Y%m%d_%H%M')}.xlsx\"\n",
    "        df.to_excel(filename, index=False)\n",
    "        print(f\"\\nğŸ‰ å®Œæˆï¼å·²ä¿å­˜ï¼š{filename} ({len(df)} æ¡)\")\n",
    "\n",
    "    def run(self, keywords, max_pages_per_kw=10, skip_list=False):\n",
    "        \"\"\"ä¸»è¿è¡Œé€»è¾‘\"\"\"\n",
    "        if os.path.exists(self.progress_file):\n",
    "            self.load_progress()\n",
    "        \n",
    "        if not skip_list:\n",
    "            start_kw_idx, start_page = self.load_crawl_state(keywords)\n",
    "            \n",
    "            print(f\"\\nğŸš€ é˜¶æ®µ1: çˆ¬å–èŒä½åˆ—è¡¨\")\n",
    "            for kw_idx in range(start_kw_idx, len(keywords)):\n",
    "                keyword = keywords[kw_idx]\n",
    "                page = start_page if kw_idx == start_kw_idx else 1\n",
    "                \n",
    "                print(f\"\\n>>> çˆ¬å–: {keyword} ({kw_idx+1}/{len(keywords)})\")\n",
    "                success = self.fetch_data(query=keyword, start_page=page, max_pages=max_pages_per_kw)\n",
    "                \n",
    "                if not success:\n",
    "                    print(\"\\nâš ï¸ è¯·æ›´æ–°Cookieåé‡æ–°è¿è¡Œ\")\n",
    "                    return\n",
    "                \n",
    "                self._simple_sleep(3.0, 6.0)\n",
    "            \n",
    "            if os.path.exists(self.state_file):\n",
    "                os.remove(self.state_file)\n",
    "        \n",
    "        start_detail_index = self.load_detail_state()\n",
    "        if self.data_list:\n",
    "            result = self.fetch_all_details_resumable(start_index=start_detail_index)\n",
    "            if result >= 0:\n",
    "                print(\"\\nâš ï¸ è¯·æ›´æ–°Cookieåé‡æ–°è¿è¡Œ\")\n",
    "                return\n",
    "        \n",
    "        self.save_final_excel()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a10626a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… æˆåŠŸåŠ è½½ Cookie\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    spider = BossSpiderAPI()\n",
    "    \n",
    "    # çˆ¬å–å…³é”®è¯åˆ—è¡¨\n",
    "    raw_keywords = [\"é“¶è¡Œ\", \"ä¿é™©\", \"é‡åŒ–\", \"è¯åˆ¸ç ”ç©¶æ‰€\", \"åŸºé‡‘ç»ç†\", \"æ•°æ®åˆ†æ\", \"äº§å“\", \"è¿è¥\", \"è´¢åŠ¡\"]\n",
    "\n",
    "    # è¿è¡Œçˆ¬è™«\n",
    "    spider.run(keywords=raw_keywords, max_pages_per_kw=10, skip_list=True)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
